{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab082ee8-c255-4f59-89ed-5aeb2ae694af",
   "metadata": {},
   "source": [
    "# EigenWatermark\n",
    "\n",
    "In this notebook, we'll implement and evaluate a simplified version of the watermark of [Tai & Mansour (2019)](https://arxiv.org/abs/1903.08238)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1d2349-1ab0-4d0e-8039-6faa318369b3",
   "metadata": {},
   "source": [
    "## Google Colab Setup\n",
    "\n",
    "The cells below handle installation and configuration for Google Colab environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8790c4-8a04-4488-a630-3c20c47c5c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "    print(\"Google Colab runtime detected\")\n",
    "except ImportError:\n",
    "    COLAB = False\n",
    "\n",
    "# Mount Google Drive to allow for persistent storage (and avoid re-downloading\n",
    "# code and data)\n",
    "if COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e510ca3-e314-47ea-a147-1b15cb9242b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# If running in Colab:\n",
    "if [[ -n \"$COLAB_RELEASE_TAG\" ]]; then\n",
    "  BASE=\"/content/drive/MyDrive\"\n",
    "  REPO_DIR=\"$BASE/wm_tutorial\"\n",
    "\n",
    "  if [[ ! -d \"$REPO_DIR\" ]]; then\n",
    "    echo \"Repo not found — cloning and installing...\"\n",
    "    mkdir -p \"$BASE\"\n",
    "    cd \"$BASE\" && git clone https://github.com/oreillyp/wm_tutorial.git\n",
    "  else\n",
    "    echo \"Repo already exists — installing without cloning...\"\n",
    "  fi\n",
    "\n",
    "  cd \"$REPO_DIR\" && pip install -e .\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04da7f6-b5c8-43c2-ab1b-c671330eb4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure `wm_tutorial` is visible\n",
    "if COLAB:\n",
    "    import site\n",
    "    site.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fbc396-001b-42cc-ae31-b2e90bafb1bc",
   "metadata": {},
   "source": [
    "<img src=\"https://assets.amazon.science/dims4/default/4be5320/2147483647/strip/true/crop/750x250+0+0/resize/1200x400!/format/webp/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2Ff7%2Fc2%2F1bfe36f74e24a5c79dd83807c5b2%2Faudio-watermark.gif._CB468320145_.gif\" width=600 height=600 />\n",
    "\n",
    "Source: [Amazon](https://www.amazon.science/blog/audio-watermarking-algorithm-is-first-to-solve-second-screen-problem-in-real-time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4ece22-cfb8-4319-910d-de18ebda8487",
   "metadata": {},
   "source": [
    "## Watermark Embedding\n",
    "\n",
    "We'll start by implementing the watermark _embedding_ algorithm, which hides watermark data in an audio signal as follows:\n",
    "\n",
    "1. Set constants `n_s` (the watermark subsequence length) and `n_r` (the number of times to repeat the subsequence)\n",
    "2. Determine the parameters of a DCT transform that will convert our audio to a time-frequency representation for watermarking, as well as the indices of the frequency bins to watermark (and resulting number of bins `n_bins`)\n",
    "3. Sample a pseudorandom binary (key) sequence `k` of length `n_s * n_r`\n",
    "4. Sample `n_s` random orthonormal vectors of dimension `n_bins`, arrange them in a sequence, and repeat `n_r` times\n",
    "5. Align vector sequence to DCT-transformed audio, scale via a `beta` paramater, and add\n",
    "6. Take inverse DCT to obtain watermarked audio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a915f7a-6129-489a-87be-b25bc863c710",
   "metadata": {},
   "source": [
    "### Set Embedding Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f2753-da75-45be-b142-36783bd19184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from audiotools import AudioSignal, STFTParams\n",
    "from audiotools.data.preprocess import create_csv\n",
    "from audiotools.data.transforms import BaseTransform, Compose, BackgroundNoise, RoomImpulseResponse\n",
    "\n",
    "from wm_tutorial.constants import MANIFESTS_DIR, DATA_DIR, ASSETS_DIR\n",
    "from wm_tutorial.util import dct, idct\n",
    "\n",
    "NOISE_DIR = DATA_DIR / \"noise-database\" / \"room\"\n",
    "RIR_DIR = DATA_DIR / \"rir-database\" / \"real\"\n",
    "\n",
    "sample_rate = 16_000\n",
    "window_length_ms = 10\n",
    "\n",
    "stft_params = STFTParams(\n",
    "    window_length=int(sample_rate * window_length_ms / 1000),\n",
    "    hop_length=int(sample_rate * window_length_ms / 1000),     # No overlap\n",
    "    window_type=\"boxcar\",\n",
    ")\n",
    "\n",
    "n_s = 2\n",
    "n_r = 100\n",
    "beta = 0.75\n",
    "\n",
    "f_min_hz = 3_000\n",
    "f_max_hz = 4_000\n",
    "\n",
    "# Compute number of embedding bins (for DCT, equal to window length)\n",
    "nyquist = sample_rate / 2\n",
    "total_bins = stft_params.window_length\n",
    "\n",
    "f_min_bin = max(0, int(f_min_hz * total_bins / nyquist))\n",
    "f_max_bin = min(total_bins, int(f_max_hz * total_bins / nyquist))\n",
    "\n",
    "n_bins = f_max_bin - f_min_bin\n",
    "\n",
    "# Because we construct orthogonal vectors via a square matrix, the number of \n",
    "# rows/columns we can take for our subsequence is at most the vector dimension\n",
    "# n_bins\n",
    "assert n_s <= n_bins\n",
    "\n",
    "print(\n",
    "    f\"Embedding parameters: \"\n",
    "    f\"subsequence length n_s={n_s}, subsequence repeats n_r={n_r}, \"\n",
    "    f\"scale beta={beta :0.2f}, embedding bins n_bins={n_bins}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a62250-ec31-44e4-a223-fcc0ec22a576",
   "metadata": {},
   "source": [
    "### Load Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ae2bd-70b4-4ef7-ab42-5d6b877671ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an example audio file\n",
    "signal = AudioSignal(ASSETS_DIR / \"audio\" / \"bryan_0.wav\").resample(sample_rate)\n",
    "signal.stft_params = stft_params\n",
    "signal.widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd6180c-6bc0-4900-8f50-468daffc8f43",
   "metadata": {},
   "source": [
    "### Obtain Orthogonal Vectors & Key Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3732cd-ab40-4174-a325-549944f94416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthogonal_matrix(d: int, seed: int):\n",
    "    \"\"\"Create a d×d random orthonormal matrix.\"\"\"\n",
    "    assert d > 0\n",
    "\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    A = torch.randn(d, d, generator=g, dtype=torch.float64)\n",
    "    Q, R = torch.linalg.qr(A, mode='reduced')\n",
    "    s = torch.sign(torch.diag(R))\n",
    "    s[s == 0] = 1\n",
    "    Q = Q * s\n",
    "    return Q\n",
    "\n",
    "\n",
    "def random_binary_sequence(l: int, seed: int):\n",
    "    \"\"\"Create a length-l random signed sequence.\"\"\"\n",
    "    assert l > 0\n",
    "    \n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    return torch.randint(0, 2, (l,), generator=g, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6b0f17-0de5-43a2-abeb-2ec8b7978a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample key sequence\n",
    "k = random_binary_sequence(n_s * n_r, seed=0)  # (n_s * n_r,)\n",
    "\n",
    "# Create subsequence of n_s orthogonal vectors, each of dimension n_bins\n",
    "m = orthogonal_matrix(n_bins, seed=0)  # (n_bins, n_bins)\n",
    "subseq = m[:n_s]                       # (n_s, n_bins)\n",
    "\n",
    "# Repeat n_r times\n",
    "subseq = subseq.repeat([n_r, 1])       # (n_s * n_r, n_bins)\n",
    "\n",
    "# Plot\n",
    "plt.imshow(subseq.T[:, :n_s * 4], aspect=\"auto\", origin=\"lower\", interpolation=\"none\")\n",
    "plt.vlines(torch.arange(0, n_s * min(n_r, 4), n_s) - 0.5, ymin=-0.5, ymax=n_bins - 0.5, colors=\"white\", linewidth=2)\n",
    "plt.title(\"Repeated embedding vector sequence\")\n",
    "plt.xlabel(\"Frames\")\n",
    "plt.ylabel(\"Bins\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebc65d1-dce9-4135-aaf5-51f9176272d9",
   "metadata": {},
   "source": [
    "### Identify Embedding Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abb7744-c280-49f6-9df4-140cfa6c44a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCT spectrogram\n",
    "spec = dct(signal)  # (n_batch, n_channels, n_freq, n_frames)\n",
    "_spec = spec.mean(1)[0]  # (n_freq, n_frames)\n",
    "\n",
    "# Embedding vectors\n",
    "start_frame = 20\n",
    "vecs = torch.zeros_like(_spec)\n",
    "vecs[f_min_bin:f_max_bin, start_frame:start_frame + n_s * n_r] = subseq.T\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow((_spec**2).log1p() + vecs, origin=\"lower\", aspect=\"auto\", interpolation=\"none\")\n",
    "\n",
    "# Embedding frequency range\n",
    "ax.axhline(f_min_bin, color='red', linewidth=2)\n",
    "ax.axhline(f_max_bin, color='red', linewidth=2)\n",
    "\n",
    "# Subsequence boundaries\n",
    "subseq_boundaries = torch.arange(start_frame, start_frame + n_s * n_r + 1, n_s)\n",
    "ax.vlines(subseq_boundaries, ymin=f_min_bin, ymax=f_max_bin, colors='white', linewidth=0.75)\n",
    "\n",
    "# Plot\n",
    "ax.set_title(\"Embedding region\")\n",
    "ax.set_xlabel(\"Frames\")\n",
    "ax.set_ylabel(\"Bins\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1c5105-1915-4220-bb0d-c6cdd38ffdd3",
   "metadata": {},
   "source": [
    "### Scale, Add, & Invert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4a3a8f-c118-43b2-a8e7-42dfe9aae27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale relative to original signal magnitude in embedding region\n",
    "orig_mag = _spec[f_min_bin:f_max_bin, start_frame:start_frame + n_s * n_r]  # (n_freq, n_s * n_r)\n",
    "scale = beta * orig_mag.norm(dim=0, p=2, keepdim=True)                      # (n_freq, n_s * n_r)\n",
    "scaled = scale * subseq.T                                                   # (n_freq, n_s * n_r)\n",
    "\n",
    "# Apply flip\n",
    "flipped = k.unsqueeze(0) * scaled                                           # (n_freq, n_s * n_r)\n",
    "\n",
    "# Add to embed\n",
    "spec[:, :, f_min_bin:f_max_bin, start_frame:start_frame + n_s * n_r] += flipped\n",
    "\n",
    "# With watermark now embedded in DCT spectrogram we can invert to obtain \n",
    "# waveform audio\n",
    "watermarked = idct(signal.clone(), spec)\n",
    "\n",
    "watermarked.widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4350186d-2a79-48eb-99bd-2cb76113ace9",
   "metadata": {},
   "source": [
    "If we want to hear the watermark, we can simply take the difference between the watermarked and unwatermarked signals!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e742bef8-ae11-4851-af99-07b4243f49e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(watermarked - signal).widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489c90f9-120b-45c1-b5d7-f3e80ab6dbb1",
   "metadata": {},
   "source": [
    "In fact, that's how we settled on an embedding frequency range of 3kHz–4kHz: using the \"before/after\" audio examples provided by the authors for this [blog post](https://www.amazon.science/blog/audio-watermarking-algorithm-is-first-to-solve-second-screen-problem-in-real-time)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093c35ab-4794-496b-ad6d-b3ab0a6e64e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "before = AudioSignal(ASSETS_DIR / \"audio\" / \"eigen_unwatermarked.wav\")\n",
    "after = AudioSignal(ASSETS_DIR / \"audio\" / \"eigen_watermarked.wav\")\n",
    "\n",
    "before.widget()\n",
    "after.widget()\n",
    "(after - before).widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae88842-1e9f-4e32-b544-c24782af185f",
   "metadata": {},
   "source": [
    "## Watermark Detection\n",
    "\n",
    "Now that we've embedded our watermark in an audio signal, how can we detect it? We'll adapt the \"self-correlation\" test proposed by Tai & Mansour, leveraging our knowledge of the secret key vector `k` to undo random flips. Detection operates as follows:\n",
    "\n",
    "1. Take the DCT spectrogram of the audio recording and isolate the subband spanning our embedding region of `f_min_hz` to `f_max_hz`\n",
    "2. For each possible watermark starting frame, take a spectrogram segment of the watermark length `n_s * n_r`and apply the flips specified by our key\n",
    "3. Divide the segment into `n_r` subsequences of length `n_s` and perform \"self-correlation\" scoring for each possible pair of subsequences\n",
    "4. Sub all self-correlation scores and move on to the next segment by advancing one frame; return all scores when finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616289b-fa15-4d7f-a33e-ff8cf0e2f9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(\n",
    "    s: AudioSignal, \n",
    "    sample_rate: int,\n",
    "    stft_params: STFTParams,\n",
    "    f_min_hz: float,\n",
    "    f_max_hz: float,\n",
    "    n_s: int,\n",
    "    n_r: int,\n",
    "    k: torch.Tensor,\n",
    "):\n",
    "    # Isolate embedding band from spectrogram\n",
    "    signal = s.clone().resample(sample_rate)\n",
    "    signal.stft_params = stft_params\n",
    "    spec = dct(signal)                        # (n_batch, n_channels, n_freq, n_frames)\n",
    "    _spec = spec.mean(1)[0]                   # (n_freq, n_frames)\n",
    "    \n",
    "    nyquist = sample_rate / 2\n",
    "    total_bins = stft_params.window_length\n",
    "\n",
    "    f_min_bin = max(0, int(f_min_hz * total_bins / nyquist))\n",
    "    f_max_bin = min(total_bins, int(f_max_hz * total_bins / nyquist))\n",
    "\n",
    "    band = _spec[f_min_bin:f_max_bin]  # (n_bins, n_frames)\n",
    "    \n",
    "    # For every length-(n_s * n_r) segment of embedding band, apply flips \n",
    "    # specified by key, correlate each pair of length-n_s flipped subsequences,\n",
    "    # and store sum\n",
    "    sums = []\n",
    "    for i in range(band.shape[-1] - (n_s * n_r) + 1):\n",
    "\n",
    "        # Select length-(n_s * n_r) segment\n",
    "        selected = band[:, i:i + n_s * n_r].clone()\n",
    "        assert selected.shape[-1] == n_s * n_r\n",
    "\n",
    "        # Apply flips, turning binary sequence into sign sequence\n",
    "        selected = selected * (2 * k.float().unsqueeze(0) - 1)\n",
    "\n",
    "        # \"Fold\" to separate subsequences\n",
    "        folded = selected.reshape(selected.shape[0], n_r, n_s)  # (n_bins, n_r, n_s)\n",
    "        folded = folded.permute(2, 1, 0)  # (n_s, n_r, n_bins)\n",
    "\n",
    "        # Self-correlation\n",
    "        normalized = folded / folded.norm(dim=2, keepdim=True).clamp_min(1e-8)\n",
    "        sim = torch.bmm(normalized, normalized.transpose(1, 2)).sum(dim=0)\n",
    "\n",
    "        # Zero redundant self-correlations\n",
    "        sim = torch.triu(sim, diagonal=1)\n",
    "\n",
    "        sums += [sim.sum().item() / (n_r * (n_r - 1) // 2)]\n",
    "    \n",
    "    return sums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a399c7f8-9070-4253-aa3c-9f3ca1a55b76",
   "metadata": {},
   "source": [
    "Let's run our detection algorithm on watermarked and unwatermarked audio and see if we observe a difference in scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b36e178-c3e1-440a-b9e6-fca6aff37076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run detection on unwatermarked audio\n",
    "scores_unwatermarked = detect(\n",
    "    signal,\n",
    "    sample_rate,\n",
    "    stft_params,\n",
    "    f_min_hz,\n",
    "    f_max_hz,\n",
    "    n_s,\n",
    "    n_r,\n",
    "    k\n",
    ")\n",
    "\n",
    "# Run detection on watermarked audio \n",
    "scores_watermarked = detect(\n",
    "    watermarked,\n",
    "    sample_rate,\n",
    "    stft_params,\n",
    "    f_min_hz,\n",
    "    f_max_hz,\n",
    "    n_s,\n",
    "    n_r,\n",
    "    k\n",
    ")\n",
    "\n",
    "# Plot detection scores\n",
    "plt.plot(scores_unwatermarked, label=\"Unwatermarked\", linewidth=1, color=\"green\")\n",
    "plt.hlines(max(scores_unwatermarked), xmin=0, xmax=len(scores_unwatermarked), linestyle=\"--\", color=\"green\", alpha=0.5)\n",
    "plt.plot(scores_watermarked, label=\"Watermarked\", linewidth=1, color=\"red\")\n",
    "plt.hlines(max(scores_watermarked), xmin=0, xmax=len(scores_watermarked), linestyle=\"--\", color=\"red\", alpha=0.5)\n",
    "plt.xlabel(\"Position\")\n",
    "plt.ylabel(\"Self-correlation score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad11cae6-5ca8-4506-93d6-f53e5fa41269",
   "metadata": {},
   "source": [
    "Indeed, there appears to be a large enough \"score gap\" to let us discriminate between watermarked and unwatermarked audio!\n",
    "\n",
    "Similar to detection, we can wrap our embedding algorithm in a single function for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54982ad-7203-4477-89c5-12b4d4cc1d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(\n",
    "    s: AudioSignal, \n",
    "    sample_rate: int,\n",
    "    stft_params: STFTParams,\n",
    "    f_min_hz: float,\n",
    "    f_max_hz: float,\n",
    "    n_s: int,\n",
    "    n_r: int,\n",
    "    beta: float,\n",
    "    seed: int,\n",
    "):\n",
    "    \n",
    "    # Isolate embedding band from spectrogram\n",
    "    signal = s.clone().resample(sample_rate)\n",
    "    signal.stft_params = stft_params\n",
    "    spec = dct(signal)                        # (n_batch, n_channels, n_freq, n_frames)\n",
    "\n",
    "    nyquist = sample_rate / 2\n",
    "    total_bins = stft_params.window_length\n",
    "\n",
    "    f_min_bin = max(0, int(f_min_hz * total_bins / nyquist))\n",
    "    f_max_bin = min(total_bins, int(f_max_hz * total_bins / nyquist))\n",
    "    n_bins = f_max_bin - f_min_bin\n",
    "    L = n_s * n_r\n",
    "\n",
    "    # Sample key sequence\n",
    "    k = random_binary_sequence(L, seed=seed)  # (n_s * n_r,)\n",
    "\n",
    "    # Create subsequence of n_s orthogonal vectors (each dim n_bins), then repeat n_r times\n",
    "    m = orthogonal_matrix(n_bins, seed=0)     # (n_bins, n_bins)\n",
    "    subseq = m[:n_s]                          # (n_s, n_bins)\n",
    "    subseq = subseq.repeat([n_r, 1])          # (n_s * n_r, n_bins)\n",
    "\n",
    "    # Choose start frame (maximum-energy subsequence)\n",
    "    band_all = spec.mean(1)[0][f_min_bin:f_max_bin, :]    # (n_bins, T)\n",
    "    T = band_all.shape[-1]\n",
    "    if T < L:\n",
    "        raise ValueError(f\"Not enough frames ({T}) for watermark length n_s * n_r = {L}.\")\n",
    "\n",
    "    # Per-frame strength\n",
    "    strength = (band_all ** 2).sum(dim=0).to(dtype=torch.float32)  # (T,)\n",
    "\n",
    "    # Rolling sum over window length L\n",
    "    kernel = torch.ones(1, 1, L, device=strength.device, dtype=strength.dtype)\n",
    "    window_sums = torch.nn.functional.conv1d(\n",
    "        strength.view(1, 1, -1), \n",
    "        kernel\n",
    "    )  # (1, 1, T - L + 1)\n",
    "    st = int(torch.argmax(window_sums).item())\n",
    "\n",
    "    # Scale per frame and embed\n",
    "    band = band_all[:, st:st + L]                                 # (n_bins, L)\n",
    "    per_frame_scale = beta * band.norm(dim=0, p=2, keepdim=True)  # (1, L)\n",
    "    scaled = subseq.T.to(band.dtype) * per_frame_scale            # (n_bins, L)\n",
    "\n",
    "    # Apply flips: turn {0,1} into {-1,+1}\n",
    "    k_tensor = torch.as_tensor(k, device=band.device, dtype=band.dtype)  # (L,)\n",
    "    sign = (2.0 * k_tensor - 1.0).unsqueeze(0)                           # (1, L)\n",
    "    flipped = scaled * sign                                              # (n_bins, L)\n",
    "\n",
    "    # Add to embed (broadcast over batch/channels)\n",
    "    spec[:, :, f_min_bin:f_max_bin, st:st + L] += flipped.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Invert\n",
    "    signal = idct(signal, spec)\n",
    "\n",
    "    # Restore original sample rate and STFT params\n",
    "    signal = signal.resample(s.sample_rate)\n",
    "    signal.stft_params = s.stft_params\n",
    "\n",
    "    # Ensure length does not change\n",
    "    signal.audio_data = signal.audio_data[..., :s.shape[-1]]\n",
    "    signal.audio_data = torch.nn.functional.pad(signal.audio_data, (0, max(0, s.shape[-1] - signal.shape[-1])))\n",
    "\n",
    "    return signal, k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f892ce-6595-43d7-87ac-aded94f8c510",
   "metadata": {},
   "source": [
    "## Evaluating Watermark Performance\n",
    "\n",
    "Now that we've got a functioning audio watermark, we'll want to evaluate its performance. In this case, we care about (1) our ability to discriminate between watermarked and unwatermarked audio under adverse conditions -- i.e., __robustness__, and (2) the degree to which our watermark preserves audio quality -- i.e., __perceptual transparency__.\n",
    "\n",
    "We'll start by implementing a simple measure of discrimination performance: the achievable true-positive detection rate given a fixed allowable false-positive detection rate, __TPR@FPR__. This reflects the fact that false positives are often costly for real-world watermarking schemes operating at scale (e.g. flagging potential deepfake videos on a social media platform for human review). Ideally, we want to maintain very high true-positive rates even at small (<1%) false-positive rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569361d8-1990-4891-b301-a94e8d0010ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpr_at_fpr(scores_true, scores_false, target_fpr: float):\n",
    "    \"\"\"\n",
    "    Compute achievable True Positive Rate (TPR) at a given False Positive Rate (FPR).\n",
    "    \"\"\"\n",
    "    # Convert to tensors\n",
    "    s_true  = torch.as_tensor(scores_true, dtype=torch.float32)\n",
    "    s_false = torch.as_tensor(scores_false, dtype=torch.float32)\n",
    "\n",
    "    # Concatenate scores and labels\n",
    "    scores = torch.cat([s_true, s_false])\n",
    "    labels = torch.cat([\n",
    "        torch.ones_like(s_true, dtype=torch.int32),\n",
    "        torch.zeros_like(s_false, dtype=torch.int32)\n",
    "    ])\n",
    "\n",
    "    # Sort scores descending\n",
    "    sorted_scores, idx = torch.sort(scores, descending=True)\n",
    "    sorted_labels = labels[idx]\n",
    "\n",
    "    # Cumulative counts\n",
    "    tp_cum = torch.cumsum(sorted_labels, dim=0)\n",
    "    fp_cum = torch.cumsum(1 - sorted_labels, dim=0)\n",
    "\n",
    "    # Totals\n",
    "    tp_total = s_true.numel()\n",
    "    fp_total = s_false.numel()\n",
    "\n",
    "    # Compute TPR and FPR\n",
    "    tpr = tp_cum.float() / tp_total\n",
    "    fpr = fp_cum.float() / fp_total\n",
    "\n",
    "    # Mask for achievable FPR\n",
    "    mask = fpr <= target_fpr\n",
    "    return tpr[mask].max().item() if mask.any() else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d1156e-316a-41f5-b5a7-e18808a16497",
   "metadata": {},
   "source": [
    "Now, we'll embed our watermark repeatedly in our tiny dataset of speech recordings (selecting a random embedding position, vector sequence, and key each time). We will then run the detection algorithm on both watermarked and unwatermarked recordings, taking the maximum detection score per recording. Finally, we will check our achievable true-positive detection rates at fixed false-positive rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63be85a2-d07c-4279-8168-de2ed910e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 500\n",
    "\n",
    "recordings = [ASSETS_DIR / \"audio\" / f\"bryan_{i}.wav\" for i in range(4)]\n",
    "\n",
    "scores_watermarked = []\n",
    "scores_unwatermarked = []\n",
    "\n",
    "for i in tqdm(range(n_examples)):\n",
    "\n",
    "    # Select random recording\n",
    "    signal = AudioSignal(random.choice(recordings)).resample(sample_rate)\n",
    "\n",
    "    # Embed watermark\n",
    "    watermarked, k = embed(\n",
    "        signal.clone(),\n",
    "        sample_rate,\n",
    "        stft_params,\n",
    "        f_min_hz,\n",
    "        f_max_hz,\n",
    "        n_s,\n",
    "        n_r,\n",
    "        beta,\n",
    "        seed=i\n",
    "    )\n",
    "\n",
    "    # Detect watermark\n",
    "    scores_watermarked += [\n",
    "        max(\n",
    "            detect(\n",
    "                watermarked,\n",
    "                sample_rate,\n",
    "                stft_params,\n",
    "                f_min_hz,\n",
    "                f_max_hz,\n",
    "                n_s,\n",
    "                n_r,\n",
    "                k\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    scores_unwatermarked += [\n",
    "        max(\n",
    "            detect(\n",
    "                signal,\n",
    "                sample_rate,\n",
    "                stft_params,\n",
    "                f_min_hz,\n",
    "                f_max_hz,\n",
    "                n_s,\n",
    "                n_r,\n",
    "                k\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"TPR @ 10% FPR: {tpr_at_fpr(scores_watermarked, scores_unwatermarked, 0.10) :0.2f}\\n\"\n",
    "    f\"TPR @ 1% FPR: {tpr_at_fpr(scores_watermarked, scores_unwatermarked, 0.01) :0.2f}\\n\"\n",
    "    f\"TPR @ 0.1% FPR: {tpr_at_fpr(scores_watermarked, scores_unwatermarked, 0.001) :0.2f}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b774ea-793f-4ed9-87b2-0db95ba82837",
   "metadata": {},
   "source": [
    "Not too shabby! But how can we measure the perceptual transparency of our watermark?\n",
    "\n",
    "The simplest metrics quantify the distance between a watermarked signal and a clean reference signal of the same length (i.e. the unwatermarked audio). These signal-level metrics can give us a good idea as to the \"magnitude\" of our watermark, but do not necessarily align well with human perception. Our first metric, __SNR__, measures the magnitude of the difference between the input and reference waveforms relative to the reference waveform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed8a98a-d931-48d3-bd0c-73f569b5610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snr(x: AudioSignal, ref: AudioSignal, eps: float = 1e-10):\n",
    "    \"\"\"Signal-to-Noise Ratio: 10 * log10( ||ref||^2 / ||x - ref||^2 )\"\"\"\n",
    "\n",
    "    assert x.sample_rate == ref.sample_rate\n",
    "    assert x.shape == ref.shape\n",
    "\n",
    "    x = x.audio_data.to(torch.float64)\n",
    "    ref = ref.audio_data.to(torch.float64)\n",
    "    \n",
    "    noise = x - ref\n",
    "    num = (ref ** 2).sum(dim=-1)               # (n_batch, n_channels)\n",
    "    den = (noise ** 2).sum(dim=-1) + eps\n",
    "    \n",
    "    snr_val = 10.0 * torch.log10(torch.clamp(num, min=eps) / den)\n",
    "    snr_val = snr_val.to(torch.float32)\n",
    "    return snr_val.mean(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edeedc3-8e0d-40ae-8612-5839dc618351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample signal\n",
    "signal = AudioSignal(random.choice(recordings)).resample(sample_rate)\n",
    "\n",
    "# Embed watermark\n",
    "watermarked, k = embed(\n",
    "    signal.clone(),\n",
    "    sample_rate,\n",
    "    stft_params,\n",
    "    f_min_hz,\n",
    "    f_max_hz,\n",
    "    n_s,\n",
    "    n_r,\n",
    "    beta,\n",
    "    seed=i\n",
    ")\n",
    "\n",
    "print(f\"SNR: {snr(watermarked, signal).item() :0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0c207e-7aad-403e-af5f-781c9ef079a6",
   "metadata": {},
   "source": [
    "While SNR's simplicity makes it interpretable and easy to implement, it has a few clear issues. One issue is its sensitivity to scaling: if two recordings differ only in their \"volume,\" even slightly, we might obtain a small SNR value despite their perceptual similarity. For example, SNR \"thinks\" that these two recordings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82944e6d-7684-4d7a-8533-80e15d759861",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled = signal * 0.75\n",
    "\n",
    "print(f\"SNR: {snr(scaled, signal).item() :0.2f}\")\n",
    "\n",
    "scaled.widget()\n",
    "signal.widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f36e14c-60aa-4f66-9637-767378b8190d",
   "metadata": {},
   "source": [
    "... are about as different as these two recordings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d6cce-622c-49c0-945b-ad490a853cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_level = snr(scaled, signal).item()\n",
    "noise_audio = signal.clone()\n",
    "noise_audio.audio_data = torch.randn_like(noise_audio.audio_data)\n",
    "noisy = signal.clone().mix(noise_audio, noise_level)\n",
    "\n",
    "print(f\"SNR: {snr(noisy, signal).item() :0.2f}\")\n",
    "\n",
    "signal.widget()\n",
    "noisy.widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3389f581-279d-44f0-ac52-3b3efec2d957",
   "metadata": {},
   "source": [
    "This particular failure mode was addressed by [Le Roux et al.](https://arxiv.org/pdf/1811.02508), who proposed the __SI-SDR__ metric to account for the effect of scale differences on the related SDR (signal-to-distortion ratio) metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2592b070-1aca-4b0e-8975-df0d1a275345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def si_sdr(x: torch.Tensor, ref: torch.Tensor, zero_mean: bool = True, eps: float = 1e-10):\n",
    "    \"\"\"\n",
    "    Scale-Invariant SDR (Le Roux et al., 2019):\n",
    "        Let x̄, s̄ be (optionally) zero-mean versions of estimate and reference.\n",
    "        alpha = <x̄, s̄> / ||s̄||^2\n",
    "        s_target = alpha * s̄\n",
    "        e = x̄ - s_target\n",
    "        SI-SDR = 10 * log10( ||s_target||^2 / ||e||^2 )\n",
    "    \"\"\"\n",
    "    assert x.sample_rate == ref.sample_rate\n",
    "    assert x.shape == ref.shape\n",
    "\n",
    "    x = x.audio_data.to(torch.float64)\n",
    "    ref = ref.audio_data.to(torch.float64)\n",
    "\n",
    "    if zero_mean:\n",
    "        # Subtract mean over time per batch and channel index\n",
    "        x = x - x.mean(dim=-1, keepdim=True)\n",
    "        ref = ref - ref.mean(dim=-1, keepdim=True)\n",
    "\n",
    "    # Project input onto reference\n",
    "    ref_energy = (ref ** 2).sum(dim=-1, keepdim=True)  # (n_batch, n_channels, 1)\n",
    "    \n",
    "    # Avoid division by zero if reference is silent\n",
    "    alpha = (x * ref).sum(dim=-1, keepdim=True) / (ref_energy + eps)\n",
    "    s_target = alpha * ref\n",
    "    e = x - s_target\n",
    "\n",
    "    num = (s_target ** 2).sum(dim=-1)           # (n_batch, n_channels)\n",
    "    den = (e ** 2).sum(dim=-1) + eps\n",
    "    si_sdr_val = 10.0 * torch.log10(torch.clamp(num, min=eps) / den)\n",
    "    si_sdr_val = si_sdr_val.to(torch.float32)\n",
    "    return si_sdr_val.mean(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee559c2-20fd-4c52-a63a-02e2667e35d5",
   "metadata": {},
   "source": [
    "Here, we can see that SI-SDR isn't tripped up by scale differences -- we get a much larger value (in deciBels) than SNR for the same scaled recording pair, indicating that the \"difference\" is minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcb8265-008f-409d-a07e-13eb96c401bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled = signal * 0.75\n",
    "\n",
    "print(f\"SI-SDR: {si_sdr(scaled, signal).item() :0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5830b9f-804c-4409-aee4-61bb5a50e488",
   "metadata": {},
   "source": [
    "There are plenty of more sophisticated signal-level metrics that attempt to mimic human perception of differences between audio signals, such as [__PESQ__](https://en.wikipedia.org/wiki/Perceptual_Evaluation_of_Speech_Quality) and [__STOI__](https://ieeexplore.ieee.org/document/5713237). However, for watermarking purposes, it's crucial to understand that _existing signal-level metrics are misaligned with human perception under many circumstances, and should not be treated as a \"gold standard\" by which to measure the perceptual transparency of watermarks!_\n",
    "\n",
    "For example, SNR, SI-SDR, PESQ, STOI, and other metrics would all consider the following pair of signals to be very different because they are _slightly misaligned in time_. Do they sound different to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b8f06e-7d86-4940-b060-884f5152adf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted = signal.clone()\n",
    "shifted.audio_data = torch.roll(shifted.audio_data, shifts=100, dims=-1)\n",
    "\n",
    "print(f\"SNR: {snr(shifted, signal).item() :0.2f}\")\n",
    "print(f\"SI-SDR: {si_sdr(shifted, signal).item() :0.2f}\")\n",
    "\n",
    "signal.widget()\n",
    "shifted.widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322fcba4-afc4-42c3-a38d-e7ef4aa2a06e",
   "metadata": {},
   "source": [
    "Where does this leave us? Signal-level metrics can still be helpful for getting a rough idea of how an audio watermark manifests -- for instance, a high SNR / SI-SDR value indicates that a watermark is low in \"magnitude\" relative to the host signal. There are also a number of [recent](https://arxiv.org/abs/2505.20741) [metrics](https://arxiv.org/abs/2304.01448) that do not require a precisely time-aligned reference signal, or even a reference signal at all. For researchers developing novel watermarking methods, using a diverse array of metrics (and clearly understanding exactly what each measures!) can help avoid overfitting to a single target.\n",
    "\n",
    "Ultimately though, the best test of perceptual transparency is a carefully conducted human listening evaluation in a standard format like [ABX](https://en.wikipedia.org/wiki/ABX_test) or [MUSHRA](https://en.wikipedia.org/wiki/MUSHRA). If we want to design an human-imperceptible watermark, there is no better judge than a human!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29986c4a-0c6f-4d6c-8c94-048fee4e6aab",
   "metadata": {},
   "source": [
    "### Watermark Robustness\n",
    "\n",
    "We've touched on perceptual transparency and measured our watermark's detection performance in a \"clean\" setting. But how will this watermark fare in the real world, where audio is often in less-than-pristine condition by the time it reaches a watermark detector? To put EigenWatermark to the test, __we'll simulate two common distortions__ -- additive noise and reverberation -- and see whether our detection performance falls off when watermarked audio is modified.\n",
    "\n",
    "Luckily for us, the `audiotools` library comes with built-in implementations of many standard audio transformations. For noise and reverberation in particular, we'll use datasets of real-world recorded background noise and room impulse responses, respectively, to model these distortions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49b03ea-6372-44be-ba3d-7266fdc6e7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AudioTools noise & reverb transforms require that noise and impulse response\n",
    "# recordings, respectively, be provided via filepaths in a .csv. We can create\n",
    "# these .csv files from our datasets using a built-in utility function\n",
    "create_csv(\n",
    "    audio_files=list((Path(NOISE_DIR)).rglob(\"*.wav\")),\n",
    "    output_csv=MANIFESTS_DIR / \"noise_room.csv\", \n",
    ")\n",
    "create_csv(\n",
    "    audio_files=list((Path(RIR_DIR)).rglob(\"*.wav\")),\n",
    "    output_csv=MANIFESTS_DIR / \"rir_real.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11be0a8a-5239-4b49-8d5f-74b23c052f87",
   "metadata": {},
   "source": [
    "Each transformation is initialized with both fixed and randomized parameters. Randomized parameters allow for realizing a large number of \"versions\" of the transformation -- e.g., with different background noise levels or room reverberation characteristics. We specify allowable values for these parameters via a tuple indicating the sampling stategy (e.g. `\"uniform\"`, `\"constant\"`, `\"choice\"`) and the allowable values/ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967199d5-a306-447f-a984-534f29fa436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = BackgroundNoise(\n",
    "    snr=(\"uniform\", 20.0, 30.0),  # Sample noise level uniformly in [10, 30]dB\n",
    "    sources=[MANIFESTS_DIR / \"noise_room.csv\"],\n",
    "    eq_amount=(\"const\", 1.0),     # Sample EQ level as a fixed value of 1.0\n",
    "    n_bands=3,\n",
    "    prob=1.0,\n",
    "    loudness_cutoff=None,\n",
    ")\n",
    "\n",
    "reverb = RoomImpulseResponse(\n",
    "    drr=(\"uniform\", 10.0, 30.0),   # Sample reverb direct-reverberant ratio uniformly in [0, 30]dB\n",
    "    sources=[MANIFESTS_DIR / \"rir_real.csv\"],\n",
    "    eq_amount=(\"const\", 1.0),     # Sample EQ level as a fixed value of 1.0\n",
    "    n_bands=6,\n",
    "    prob=1.0,\n",
    "    use_original_phase=False,\n",
    "    offset=0.0,\n",
    "    duration=1.0,\n",
    ")\n",
    "\n",
    "# We can combine multiple transforms in sequence!\n",
    "noise_and_reverb = Compose(noise, reverb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f48caf1-2247-46dc-9226-04f16ddaee6e",
   "metadata": {},
   "source": [
    "We can then apply a transformation to audio in two steps:\n",
    "\n",
    "1. Sample randomized parameters via `.instantiate()` (passing the signal to be transformed and a random seed) to obtain a dictionary that deterministically specifies the sampled transformation\n",
    "2. Pass the signal and dictionary of sampled parameters  to `.transform()` to apply the transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a1f55b-e964-432c-9871-0d95db73a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.widget()\n",
    "\n",
    "noise_kwargs = noise.instantiate(1, signal)\n",
    "print(\"Sampled noise parameters:\", noise_kwargs)\n",
    "out_noise = noise.transform(signal.clone(), **noise_kwargs)\n",
    "out_noise.widget()\n",
    "\n",
    "reverb_kwargs = reverb.instantiate(1, signal)\n",
    "print(\"Sampled reverb parameters:\", reverb_kwargs)\n",
    "out_reverb = reverb.transform(signal.clone(), **reverb_kwargs)\n",
    "out_reverb.widget()\n",
    "\n",
    "both_kwargs = noise_and_reverb.instantiate(1, signal)\n",
    "print(\"Sampled noise+reverb parameters:\", both_kwargs)\n",
    "out_both = noise_and_reverb.transform(signal.clone(), **both_kwargs)\n",
    "out_both.widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a072f-c143-483e-a254-58350d1fcf68",
   "metadata": {},
   "source": [
    "Now, let's re-run our evaluation, but apply random noise and reverb to each recording before performing detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217cb99a-d1d5-46f3-9e54-11d6baf594ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 500\n",
    "\n",
    "recordings = [ASSETS_DIR / \"audio\" / f\"bryan_{i}.wav\" for i in range(4)]\n",
    "\n",
    "scores_watermarked = []\n",
    "scores_unwatermarked = []\n",
    "\n",
    "for i in tqdm(range(n_examples)):\n",
    "\n",
    "    # Select random recording\n",
    "    signal = AudioSignal(random.choice(recordings)).resample(sample_rate)\n",
    "    \n",
    "    # Embed watermark\n",
    "    watermarked, k = embed(\n",
    "        signal.clone(),\n",
    "        sample_rate,\n",
    "        stft_params,\n",
    "        f_min_hz,\n",
    "        f_max_hz,\n",
    "        n_s,\n",
    "        n_r,\n",
    "        beta=beta,\n",
    "        seed=i\n",
    "    )\n",
    "\n",
    "    # Apply random transformation to both unwatermarked and watermarked signal\n",
    "    tfm_kwargs = noise_and_reverb.instantiate(1, signal)\n",
    "    tfm_signal = noise_and_reverb.transform(signal.clone(), **tfm_kwargs)\n",
    "    tfm_watermarked = noise_and_reverb.transform(watermarked.clone(), **tfm_kwargs)\n",
    "    \n",
    "    # Detect watermark\n",
    "    scores_watermarked += [\n",
    "        max(\n",
    "            detect(\n",
    "                tfm_watermarked,\n",
    "                sample_rate,\n",
    "                stft_params,\n",
    "                f_min_hz,\n",
    "                f_max_hz,\n",
    "                n_s,\n",
    "                n_r,\n",
    "                k\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    scores_unwatermarked += [\n",
    "        max(\n",
    "            detect(\n",
    "                tfm_signal,\n",
    "                sample_rate,\n",
    "                stft_params,\n",
    "                f_min_hz,\n",
    "                f_max_hz,\n",
    "                n_s,\n",
    "                n_r,\n",
    "                k\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"TPR @ 10% FPR: {tpr_at_fpr(scores_watermarked, scores_unwatermarked, 0.10) :0.2f}\\n\"\n",
    "    f\"TPR @ 1% FPR: {tpr_at_fpr(scores_watermarked, scores_unwatermarked, 0.01) :0.2f}\\n\"\n",
    "    f\"TPR @ 0.1% FPR: {tpr_at_fpr(scores_watermarked, scores_unwatermarked, 0.001) :0.2f}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffb5aff-a1d1-4849-a951-ac4348a479e8",
   "metadata": {},
   "source": [
    "And look at that -- our watermark maintains strong performance even under simulated acoustic distortions! This isn't just luck. Tai & Mansour explicitly designed their watermark for speech in noisy, reverberant environments. Key choices include:\n",
    "* Embedding the watermark in a narrow frequency band (3-4kHz) that is often pronounced speech recordings\n",
    "* Leveraging _self-correlation_ rather than _cross-correlation_ to embed and detect the watermark, as stationary additive noise and time-invariant filters (which reverberation can be modeled as) do not interfere with spectral self-similarity between different segments of the signal.\n",
    "\n",
    "EigenWatermark was originally proposed to prevent Amazon Alexa from \"waking\" when the name appeared in television commercials. If the watermark were embedded in the audio track of a commercial, with the watermark key distributed to Alexa devices, they could be configured to prevent \"waking\" when the watermark was detected. However, there's nothing to stop this watermarking approach from [also being applied to synthetic speech identification](https://interactiveaudiolab.github.io/assets/papers/oreilly_jin_su_pardo_watermark.pdf).\n",
    "\n",
    "However, EigenWatermark isn't universally robust. Let's try a couple of audio transformations the original authors didn't consider due to their irrelevance in the wakeword-detection scenario: spectral gating and speed change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cfa465-7b58-452f-b9c5-e98b3d4eda8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiotools.core.util import sample_from_dist\n",
    "from audiotools.data.transforms import MaskLowMagnitudes\n",
    "from numpy.random import RandomState\n",
    "\n",
    "class Speed(BaseTransform):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        factor: tuple = (\"choice\", (0.99, 1.01)),\n",
    "        name: str = None,\n",
    "        prob: float = 1.0,\n",
    "    ):\n",
    "        super().__init__(name=name, prob=prob)\n",
    "        \n",
    "        self.factor = factor\n",
    "\n",
    "    def _instantiate(self, state: RandomState):\n",
    "\n",
    "        factor = sample_from_dist(self.factor, state)\n",
    "        return {\"factor\": factor}\n",
    "\n",
    "    def _transform(self, signal, factor):\n",
    "\n",
    "        if isinstance(factor, (float, int)):\n",
    "            factor = [factor]\n",
    "\n",
    "        out = signal.clone()\n",
    "        \n",
    "        for i, _factor in enumerate(factor):\n",
    "\n",
    "            src_rate = int(factor * signal.sample_rate)\n",
    "            tgt_rate = int(signal.sample_rate)\n",
    "            \n",
    "            # Keep GCD of source and target sample rates reasonably large to \n",
    "            # limit resampling kernel size\n",
    "            assert not tgt_rate % 50\n",
    "            src_rate = (src_rate // 50 * 50)\n",
    "\n",
    "            _out = out[i].clone()\n",
    "            _out.sample_rate = src_rate\n",
    "            _out = _out.resample(tgt_rate)\n",
    "            _len = min(out.shape[-1], _out.shape[-1])\n",
    "            out.audio_data[i, :, :_len] = _out.audio_data[..., :_len]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1939eff-6cda-49f6-92e7-92587a4a0a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectralgate = MaskLowMagnitudes(\n",
    "    db_cutoff=(\"const\", -20),\n",
    "    prob=1.0,\n",
    ")\n",
    "speed = Speed()\n",
    "attack = Compose(speed, spectralgate)\n",
    "\n",
    "\n",
    "tfm_kwargs = spectralgate.instantiate(0, signal)\n",
    "out = spectralgate.transform(signal.clone(), **tfm_kwargs)\n",
    "out.widget()\n",
    "\n",
    "\n",
    "tfm_kwargs = speed.instantiate(0, signal)\n",
    "out = speed.transform(signal.clone(), **tfm_kwargs)\n",
    "out.widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1f0653-3998-4e92-be70-77797eef70c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 500\n",
    "\n",
    "recordings = [ASSETS_DIR / \"audio\" / f\"bryan_{i}.wav\" for i in range(4)]\n",
    "\n",
    "scores_watermarked = []\n",
    "scores_unwatermarked = []\n",
    "\n",
    "for i in tqdm(range(n_examples)):\n",
    "\n",
    "    # Select random recording\n",
    "    signal = AudioSignal(random.choice(recordings)).resample(sample_rate)\n",
    "    \n",
    "    # Embed watermark\n",
    "    watermarked, k = embed(\n",
    "        signal.clone(),\n",
    "        sample_rate,\n",
    "        stft_params,\n",
    "        f_min_hz,\n",
    "        f_max_hz,\n",
    "        n_s,\n",
    "        n_r,\n",
    "        beta=beta,\n",
    "        seed=i\n",
    "    )\n",
    "\n",
    "    # Apply random transformation to both unwatermarked and watermarked signal\n",
    "    tfm_kwargs = attack.instantiate(1, signal)\n",
    "    tfm_signal = attack.transform(signal.clone(), **tfm_kwargs)\n",
    "    tfm_watermarked = attack.transform(watermarked.clone(), **tfm_kwargs)\n",
    "    \n",
    "    # Detect watermark\n",
    "    scores_watermarked += [\n",
    "        max(\n",
    "            detect(\n",
    "                tfm_watermarked,\n",
    "                sample_rate,\n",
    "                stft_params,\n",
    "                f_min_hz,\n",
    "                f_max_hz,\n",
    "                n_s,\n",
    "                n_r,\n",
    "                k\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    scores_unwatermarked += [\n",
    "        max(\n",
    "            detect(\n",
    "                tfm_signal,\n",
    "                sample_rate,\n",
    "                stft_params,\n",
    "                f_min_hz,\n",
    "                f_max_hz,\n",
    "                n_s,\n",
    "                n_r,\n",
    "                k\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"TPR @ 10% FPR: {tpr_at_fpr(scores_watermarked, scores_unwatermarked, 0.10) :0.2f}\\n\"\n",
    "    f\"TPR @ 1% FPR: {tpr_at_fpr(scores_watermarked, scores_unwatermarked, 0.01) :0.2f}\\n\"\n",
    "    f\"TPR @ 0.1% FPR: {tpr_at_fpr(scores_watermarked, scores_unwatermarked, 0.001) :0.2f}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223f004f-f23d-4226-9d04-8c8eea6f5e85",
   "metadata": {},
   "source": [
    "And at last, we've found the limit of this watermark's robustness. What does it take to break it? Let's have a listen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89de5a6-0b39-4fab-9111-d7ac526ac1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.widget()\n",
    "watermarked.widget()\n",
    "tfm_signal.widget()\n",
    "tfm_watermarked.widget()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genmodels",
   "language": "python",
   "name": "genmodels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
