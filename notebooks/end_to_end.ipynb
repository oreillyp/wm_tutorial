{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29506dba-25ba-4b99-8741-3a184fe6b661",
   "metadata": {},
   "source": [
    "# End-to-End Neural Network Watermark\n",
    "\n",
    "In this notebook, we'll implement and evaluate a simplified end-to-end neural network-based audio watermarking system. To emphasize the generality of the principles at play here, we'll define a \"vanilla\" transformer-based architecture rather than trying to adopt the architecture of a specific published watermarking system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfc3858-7a13-4266-a4a5-d69856b841e6",
   "metadata": {},
   "source": [
    "## Google Colab Setup\n",
    "\n",
    "The cells below handle installation and configuration for Google Colab environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c637b7b-94e5-499a-8fc8-1507ad54d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "    print(\"Google Colab runtime detected\")\n",
    "except ImportError:\n",
    "    COLAB = False\n",
    "\n",
    "# Mount Google Drive to allow for persistent storage (and avoid re-downloading\n",
    "# code and data)\n",
    "if COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df84c0e3-acf9-4a50-8089-f29944eef478",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# If running in Colab:\n",
    "if [[ -n \"$COLAB_RELEASE_TAG\" ]]; then\n",
    "  BASE=\"/content/drive/MyDrive\"\n",
    "  REPO_DIR=\"$BASE/wm_tutorial\"\n",
    "\n",
    "  if [[ ! -d \"$REPO_DIR\" ]]; then\n",
    "    echo \"Repo not found — cloning and installing...\"\n",
    "    mkdir -p \"$BASE\"\n",
    "    cd \"$BASE\" && git clone https://github.com/oreillyp/wm_tutorial.git\n",
    "  else\n",
    "    echo \"Repo already exists — installing without cloning...\"\n",
    "  fi\n",
    "\n",
    "  cd \"$REPO_DIR\" && pip install -e .\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3188f2bc-2122-4d7d-8319-c16802c72de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure `wm_tutorial` is visible\n",
    "if COLAB:\n",
    "    import site\n",
    "    site.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd0eda2-d760-4490-90c7-3c5bc69d9add",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Transformers for Sequence Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6347638e-74a0-4532-93ea-86341120e792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from audiotools import AudioSignal, STFTParams\n",
    "from audiotools.data.datasets import AudioLoader, AudioDataset\n",
    "from audiotools.data.preprocess import create_csv\n",
    "from audiotools.data.transforms import BaseTransform, Compose, BackgroundNoise, RoomImpulseResponse, MaskLowMagnitudes, Equalizer\n",
    "\n",
    "from wm_tutorial.constants import DATA_DIR, ASSETS_DIR, MANIFESTS_DIR\n",
    "from wm_tutorial.util import count_parameters, collate, tpr_at_fpr, snr, si_sdr\n",
    "from wm_tutorial.nn.transformer import Transformer\n",
    "from wm_tutorial.nn.message import MessageEmbedding, MessageBlock\n",
    "from wm_tutorial.tfm import Noise, Reverb, Speed\n",
    "\n",
    "NOISE_DIR = DATA_DIR / \"noise-database\" / \"room\"\n",
    "RIR_DIR = DATA_DIR / \"rir-database\" / \"real\"\n",
    "\n",
    "sample_rate = 16_000\n",
    "window_length_ms = 50\n",
    "max_len_s = 5.0\n",
    "n_train_steps = 10_000\n",
    "batch_size = 32  # Adjust as needed to account for your GPU memory!\n",
    "\n",
    "# Watermark message length, i.e. capacity\n",
    "n_bits = 16\n",
    "\n",
    "# Check if GPU available\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# STFT parameters\n",
    "stft_params = STFTParams(\n",
    "    window_length=int(sample_rate * window_length_ms / 1000),\n",
    "    hop_length=int(sample_rate * window_length_ms * 0.75 / 1000),\n",
    "    window_type=\"tukey\",\n",
    ")\n",
    "\n",
    "# Transformer config\n",
    "config = {\n",
    "    \"model_dim\": 256,\n",
    "    \"num_heads\": 8,\n",
    "    \"num_layers\": 8,\n",
    "    \"bias\": False,\n",
    "    \"dropout\": 0.1,\n",
    "    \"max_len\": int(max_len_s * sample_rate / stft_params.hop_length) + 2,\n",
    "    \"dim_feedforward\": 1024,\n",
    "    \"pos_enc\": \"absolute\",\n",
    "}\n",
    "\n",
    "# Initialize a small transformer\n",
    "tfmr = Transformer(**config)\n",
    "count_parameters(tfmr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b22a19-459f-415e-88f9-a4cd6e6b336d",
   "metadata": {},
   "source": [
    "Our transformer processes sequential data of shape `(n_batch, seq_len, model_dim)`, producing outputs of the same shape as its inputs. Ideally, this processing extracts/contextualizes/enriches information within the sequence relevant to our training task. For the purposes of this tutorial, we won't be diving too deep into the transformer architecture, and will be treating it as a \"black-box\" building block for our watermark embedding and detection algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013b6db7-553b-45f1-9800-a646b83fcc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a random input sequence\n",
    "x = torch.randn(1, config[\"max_len\"], config[\"model_dim\"])\n",
    "out = tfmr(x, mask=None)\n",
    "\n",
    "print(f\"Input shape: {x.shape}, output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c3cf06-0017-41ff-bee2-02197de73aa3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Representing Watermark Messages\n",
    "\n",
    "In end-to-end neural network-based watermarking systems, watermark keys or messages typically take the form of fixed-length binary vectors. Because these systems operate by predicting a message directly from audio -- i.e., in a _steganographic_ manner -- the message size is essentially the system's information __capacity__. For instance, a system capable of embedding length-16 binary vectors in audio and reconstructing them at detection time has a 16-bit capacity.\n",
    "\n",
    "How do we pass this message to our watermark embedding network? Most neural network architectures are designed to operate on _continuous_ vector representations of data, not booleans. We can take a page out of the book of large language models, which are tasked with representing sequences of text \"tokens\" (in reality, integer indices into a vocabulary). What if we did something similar with binary vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd94ed3-b83a-4384-b14f-de3c01a0cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a watermark message (binary vector)\n",
    "msg = torch.randint(0, 2, (n_bits,))\n",
    "\n",
    "# Create an \"embedding\" lookup table that maps a binary value {0, 1} to one of two corresponding continuous vectors\n",
    "emb_table = torch.nn.Embedding(2, config[\"model_dim\"])\n",
    "\n",
    "embedded = emb_table(msg)\n",
    "\n",
    "print(\n",
    "    f\"Original message: {msg.tolist()}\\n\"\n",
    "    f\"Message tensor shape: {msg.shape}\\n\"\n",
    "    f\"Embedded message tensor shape: {embedded.shape}\\n\"\n",
    ")\n",
    "\n",
    "plt.imshow(\n",
    "    msg.unsqueeze(0).repeat(config[\"model_dim\"], 1), \n",
    "    origin=\"lower\", aspect=\"auto\", interpolation=\"none\"\n",
    ")\n",
    "plt.title(\"Binary message\")\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"Message sequence idx\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(\n",
    "    embedded.detach().T, \n",
    "    origin=\"lower\", aspect=\"auto\", interpolation=\"none\"\n",
    ")\n",
    "plt.title(\"Embedded message\")\n",
    "plt.xlabel(\"Message sequence idx\")\n",
    "plt.ylabel(\"Channel dim\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96bccfd-fd8d-4274-824c-91d879541716",
   "metadata": {},
   "source": [
    "Now our message is a sequence of continuous vectors, which is a good first step. But how do we distinguish between a `0` in the first position and a `0` in the tenth position? Both vectors look the same -- there is no explicit encoding of positional information in this representation! While some architectures (e.g. convolutional neural networks) will impart the required positional information automatically, this isn't the case with transformers.\n",
    "\n",
    "One solution is to just apply standard positional encoding (e.g. sinusoidal, RoPE) to our embedded message sequence. Another approach, used by Fernandez et al. in the [AudioSeal watermarking system](https://arxiv.org/abs/2401.17264), is to use a different learnable pair of embedding vectors per sequence index in the message. That is, the first index's 0/1 vectors are separate from the second index's 0/1 vectors, and so on. It is common to sum the resulting embedding sequence along the message dimension to obtain a single vector that represents all bit values and positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b24377-2e2e-4a9b-8354-5b6226842fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an embedding table with two entries per bit\n",
    "emb_table_with_pos = torch.nn.Embedding(2 * n_bits, config[\"model_dim\"])\n",
    "\n",
    "# Modify message so that each entry now holds an index into the correct\n",
    "# bit value / position index in our embedding table\n",
    "msg_with_pos = msg + 2 * torch.arange(n_bits)\n",
    "\n",
    "# Get bit value / position embeddings\n",
    "embedded_with_pos = emb_table_with_pos(msg_with_pos).sum(0)\n",
    "\n",
    "print(\n",
    "    f\"Original message: {msg.tolist()}\\n\"\n",
    "    f\"Message with offsets: {msg_with_pos.tolist()}\\n\"\n",
    "    f\"Summed embedded message tensor shape: {embedded_with_pos.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f687a960-9881-4fca-81f2-b3500b0b3149",
   "metadata": {},
   "source": [
    "In practice, we could represent our message in a number of ways. For now though, we'll stick with this format and move on to the process of actually \"hiding\" our messages. \n",
    "\n",
    "How can we modify a given sequence of values (e.g. representing audio data) to incorporate information from our message? One simple mechanism is __cross-attention__, which is similar to the self-attention operation at the core of the transformer architecture. Whereas self-attention defines and operates on relationships between elements within a single sequence, cross-attention defines and operates on relationships between two sequences. In our case, we will allow each position in a sequence representing our audio data to \"attend to\" a representation of our watermark message. A nice thing about this method is that it works whether we squeeze our message down into a single vector (like in AudioSeal) or keep it as a sequence of values (as long as we remember to sprinkle in some positional encoding!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8f71e5-2052-40ba-848a-c528d32f27ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cross-attention operation:\n",
    "# 1. Project \"audio data\" sequence to obtain query sequence\n",
    "# 2. Project \"message\" sequence to obtain key and value sequences\n",
    "# 3. Split into multiple \"heads\" along hidden dimension\n",
    "# 4. Perform standard attention using keys/queries/values\n",
    "k_proj = torch.nn.Linear(config[\"model_dim\"], config[\"model_dim\"])\n",
    "v_proj = torch.nn.Linear(config[\"model_dim\"], config[\"model_dim\"])\n",
    "q_proj = torch.nn.Linear(config[\"model_dim\"], config[\"model_dim\"])\n",
    "\n",
    "# 1.\n",
    "q = q_proj(x)                                 # (n_batch, seq_len, model_dim)\n",
    "\n",
    "# 2.\n",
    "k = k_proj(embedded_with_pos.view(1, 1, -1))  # (n_batch, 1, model_dim)\n",
    "v = v_proj(embedded_with_pos.view(1, 1, -1))  # (n_batch, 1, model_dim)\n",
    "\n",
    "# 3.\n",
    "n_batch, tgt_len, _ = q.shape\n",
    "_, src_len, model_dim = k.shape\n",
    "n_heads = 2\n",
    "head_dim = model_dim // n_heads\n",
    "\n",
    "q = q.view(n_batch, tgt_len, n_heads, head_dim).transpose(1, 2)  # (n_batch, n_heads, seq_len, head_dim)\n",
    "k = k.view(n_batch, src_len, n_heads, head_dim).transpose(1, 2)  # (n_batch, n_heads, 1, head_dim)\n",
    "v = v.view(n_batch, src_len, n_heads, head_dim).transpose(1, 2)  # (n_batch, n_heads, 1, head_dim)\n",
    "\n",
    "# 4.\n",
    "cross_attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "    q,\n",
    "    k,\n",
    "    v,\n",
    "    attn_mask=None,\n",
    ")  # (n_batch, n_heads, seq_len, head_dim)\n",
    "cross_attn_output = cross_attn_output.transpose(1, 2).contiguous().view(n_batch, tgt_len, -1)  # (n_batch, seq_len, model_dim)\n",
    "\n",
    "plt.imshow(x[0].T, aspect=\"auto\", origin=\"lower\", interpolation=\"none\")\n",
    "plt.xlabel(\"Frames\")\n",
    "plt.ylabel(\"Channel dim\")\n",
    "plt.title(\"\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(cross_attn_output[0].T.detach(), aspect=\"auto\", origin=\"lower\", interpolation=\"none\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14d83e2-f53f-44b3-bd65-b47b131a3759",
   "metadata": {},
   "source": [
    "We can interleave many such cross-attention layers within our network to repeatedly embed aspects of the watermark message. Which brings us to..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389ab66a-68b6-44c7-99ce-8de182933552",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## The Watermark Embedder\n",
    "\n",
    "The watermark embedding network is tasked with modifying a given audio signal to incorporate a given message. We will do so in the frequency domain, modifying a magnitude spectrogram representation of our audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e49985-b34a-4e88-b7fa-d5575d530c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        sample_rate: int,\n",
    "        stft_params: STFTParams,\n",
    "        n_bits: int = 8,\n",
    "        gate: str = \"sigmoid\",\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Transformer backbone\n",
    "        self.transformer = Transformer(*args, **kwargs)\n",
    "\n",
    "        # Construct message-embedding cross-attention blocks to interleave \n",
    "        # within transformer backbone\n",
    "        cross_attn = [\n",
    "            MessageBlock(gate=gate, *args, **kwargs) \n",
    "            for _ in range(self.transformer.num_layers)\n",
    "        ]\n",
    "\n",
    "        self.cross_attn = torch.nn.ModuleList(cross_attn)\n",
    "\n",
    "        # AudioSeal-style message embedding from above, with some extra bells\n",
    "        # and whistles\n",
    "        self.msg_emb = MessageEmbedding(n_bits, self.transformer.model_dim)\n",
    "\n",
    "        # Project inputs to match hidden dimension\n",
    "        self.in_proj = torch.nn.Linear(\n",
    "            stft_params.window_length // 2 + 1, self.transformer.model_dim,\n",
    "        )\n",
    "        \n",
    "        # Project outputs to match spectrogram dimension\n",
    "        self.out_proj = torch.nn.Linear(\n",
    "            self.transformer.model_dim, stft_params.window_length // 2 + 1,\n",
    "        )\n",
    "        \n",
    "        # Ensure non-negative outputs\n",
    "        self.softplus = torch.nn.Softplus()\n",
    "        \n",
    "        self.sample_rate = sample_rate\n",
    "        self.stft_params = stft_params\n",
    "        self.n_bits = n_bits\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        signal: AudioSignal, \n",
    "        msg: torch.Tensor,\n",
    "        signal_lengths: torch.Tensor = None,\n",
    "        msg_mask: torch.Tensor = None,\n",
    "    ):\n",
    "        \n",
    "        assert msg.ndim == 2  # (n_batch, msg_len)\n",
    "        n_batch = signal.batch_size\n",
    "\n",
    "        # If no signal lengths provided, assume full-length\n",
    "        if signal_lengths is None:\n",
    "            signal_lengths = torch.full(\n",
    "                (n_batch,),\n",
    "                signal.signal_length, \n",
    "                dtype=torch.long, \n",
    "                device=signal.device\n",
    "            )\n",
    "\n",
    "        # Compute spectrogram of input audio\n",
    "        watermarked = signal.clone().resample(self.sample_rate)\n",
    "        watermarked.stft_params = self.stft_params\n",
    "        watermarked.stft()\n",
    "\n",
    "        mag = watermarked.magnitude  # (n_batch, n_channels, n_freq, n_frames)\n",
    "        phase = watermarked.phase    # (n_batch, n_channels, n_freq, n_frames)\n",
    "\n",
    "        # Project to match model dimension\n",
    "        x = self.in_proj(mag.mean(1).transpose(1, 2))  # (n_batch, n_frames, model_dim)\n",
    "\n",
    "        # Convert signal lengths to spectrogram frame resolution\n",
    "        lengths = signal_lengths * (x.shape[1] / signal.signal_length)\n",
    "        lengths = lengths.clamp(max=x.shape[1]).long()\n",
    "\n",
    "        # Construct padding mask to prevent attention to padded frames\n",
    "        lengths_expanded = lengths.to(x.device).view(n_batch, 1, 1)  # (n_batch, 1, 1)\n",
    "        range_tensor = torch.arange(x.shape[1], device=x.device).view(1, 1, x.shape[1])\n",
    "        padding_mask = (range_tensor < lengths_expanded).repeat(1, x.shape[1], 1)  # (n_batch, n_frames, n_frames)\n",
    "        \n",
    "        # Embed message\n",
    "        msg_emb = self.msg_emb(msg, mask=msg_mask).unsqueeze(1)  # (n_batch, 1, model_dim)\n",
    "        \n",
    "        # DEBUG: issue appears to be here, at least in part\n",
    "        # Pass through model\n",
    "        for i, (attn_layer, cross_attn_layer) in enumerate(\n",
    "            zip(self.transformer.layers, self.cross_attn)\n",
    "        ):\n",
    "            x = cross_attn_layer(x, msg_emb)\n",
    "            x = attn_layer(x, padding_mask)  # (n_batch, n_frames, model_dim)\n",
    "\n",
    "        # Project to obtain a multiplicative magnitude spectrogram mask and\n",
    "        # apply softplus to ensure nonnegative\n",
    "        out = self.out_proj(x)  # (n_batch, n_frames, n_freq)\n",
    "        out = self.softplus(out)\n",
    "\n",
    "        # Apply multiplicative mask\n",
    "        mag = mag * out.unsqueeze(1).transpose(2, 3)  # (n_batch, n_channels, n_freq, n_frames)\n",
    "\n",
    "        # Invert spectrogram to obtain audio\n",
    "        watermarked.magnitude = mag\n",
    "        watermarked.phase = phase\n",
    "        watermarked = watermarked.istft(\n",
    "            window_length=self.stft_params.window_length,\n",
    "            hop_length=self.stft_params.hop_length,\n",
    "            window_type=self.stft_params.window_type,\n",
    "            match_stride=self.stft_params.match_stride,\n",
    "            length=watermarked.signal_length,\n",
    "        )\n",
    "\n",
    "        # Restore sample rate\n",
    "        watermarked.resample(signal.sample_rate)\n",
    "        watermarked.stft_params = signal.stft_params\n",
    "\n",
    "        # Ensure length matches\n",
    "        watermarked.audio_data = watermarked.audio_data[..., :signal.signal_length]\n",
    "        watermarked.audio_data = torch.nn.functional.pad(\n",
    "            watermarked.audio_data, \n",
    "            (0, max(0, signal.signal_length - watermarked.signal_length))\n",
    "        )\n",
    "\n",
    "        return watermarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2139b28f-04f3-4890-a797-34f42a601c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding network\n",
    "embedder = Embedder(sample_rate, stft_params, n_bits, **config)\n",
    "print(f\"Embedder parameters: {count_parameters(embedder)}\")\n",
    "\n",
    "# Load audio data\n",
    "signal = AudioSignal(ASSETS_DIR / \"audio\" / \"bryan_0.wav\").resample(sample_rate)\n",
    "signal = signal[..., :int(signal.sample_rate * max_len_s)]\n",
    "signal.widget()\n",
    "\n",
    "# Embed a random watermark message in signal\n",
    "watermarked = embedder(signal, torch.randint(0, 2, (signal.batch_size, n_bits))).detach()\n",
    "watermarked.widget()\n",
    "\n",
    "# Examine difference between original and \"watermarked\" signal\n",
    "(watermarked - signal).widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a9ebc9-0ce2-4fc3-9748-75056b21f1c3",
   "metadata": {},
   "source": [
    "Note that we haven't actually watermarked anything here -- our network is randomly initialized and has not yet been trained to hide messages in a recoverable manner! How can we train our embedder to do this? We'll need to start by defining..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983fccf2-3293-4caf-82c5-8490456addc9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## The Watermark Detector\n",
    "\n",
    "Our detector inverts the processing performed by the embedder: given an audio signal, it predicts a fixed-length watermark vector. To map arbitrary-length audio recordings to watermark vector predictions, we'll use __attention pooling__. If we prepend a learnable latent vector to our encoded audio sequence and use our transformer's output from only this sequence position to predict encoded watermark vectors, our detector network will naturally learn to pool relevant information from across sequences into this position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ec091c-24ed-4f08-a4f9-0ac5ce37fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        sample_rate: int,\n",
    "        stft_params: STFTParams,\n",
    "        n_bits: int = 8,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Transformer backbone\n",
    "        self.transformer = Transformer(*args, **kwargs)\n",
    "\n",
    "        # Learnable \"pool\" embedding\n",
    "        self.pool_emb = torch.nn.Parameter(\n",
    "            torch.randn(self.transformer.model_dim),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        \n",
    "        # Project inputs to match hidden dimension\n",
    "        self.in_proj = torch.nn.Linear(\n",
    "            stft_params.window_length // 2 + 1, self.transformer.model_dim,\n",
    "        )\n",
    "        \n",
    "        # Project outputs to obtain watermark vector predictions\n",
    "        self.out_proj = torch.nn.Linear(\n",
    "            self.transformer.model_dim, n_bits,\n",
    "        )\n",
    "        \n",
    "        self.sample_rate = sample_rate\n",
    "        self.stft_params = stft_params\n",
    "        self.n_bits = n_bits\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        signal: AudioSignal, \n",
    "        signal_lengths: torch.Tensor = None,\n",
    "    ):\n",
    "        \n",
    "        n_batch = signal.batch_size\n",
    "\n",
    "        # If no signal lengths provided, assume full-length\n",
    "        if signal_lengths is None:\n",
    "            signal_lengths = torch.full(\n",
    "                (n_batch,),\n",
    "                signal.signal_length, \n",
    "                dtype=torch.long, \n",
    "                device=signal.device\n",
    "            )\n",
    "\n",
    "        # Compute spectrogram of input audio\n",
    "        orig_signal_length = signal.signal_length\n",
    "        signal = signal.clone().resample(self.sample_rate)\n",
    "        signal.stft_params = self.stft_params\n",
    "        signal.stft()\n",
    "\n",
    "        mag = signal.magnitude  # (n_batch, n_channels, n_freq, n_frames)\n",
    "\n",
    "        # Project to match model dimension\n",
    "        x = self.in_proj(mag.mean(1).transpose(1, 2))  # (n_batch, n_frames, model_dim)\n",
    "\n",
    "        # Convert signal lengths to spectrogram frame resolution\n",
    "        lengths = signal_lengths * (x.shape[1] / orig_signal_length)\n",
    "        lengths = lengths.clamp(max=x.shape[1]).long()\n",
    "\n",
    "        # Prepend learnable \"pool\" embedding and adjust lengths accordingly\n",
    "        x = torch.cat(\n",
    "            [self.pool_emb.view(1, 1, -1).repeat(n_batch, 1, 1), x],\n",
    "            dim=1,\n",
    "        )  # (n_batch, n_frames + 1, model_dim)\n",
    "        lengths += 1  # (n_batch,)\n",
    "        \n",
    "        # Construct padding mask to prevent attention to padded frames\n",
    "        lengths_expanded = lengths.to(x.device).view(n_batch, 1, 1)  # (n_batch, 1, 1)\n",
    "        range_tensor = torch.arange(x.shape[1], device=x.device).view(1, 1, x.shape[1])\n",
    "        padding_mask = (range_tensor < lengths_expanded).repeat(1, x.shape[1], 1)  # (n_batch, n_frames + 1, n_frames + 1)\n",
    "        \n",
    "        # Pass through model\n",
    "        for i, attn_layer in enumerate(self.transformer.layers):\n",
    "            x = attn_layer(x, padding_mask)  # (n_batch, n_frames + 1, model_dim)\n",
    "\n",
    "        # Project output of first frame (corresponding to \"pool\" embedding) to obtain\n",
    "        # watermark prediction\n",
    "        out = self.out_proj(x[:, 0, :])  # (n_batch, n_bits)\n",
    "\n",
    "        # Bound to [0, 1]\n",
    "        return torch.sigmoid(out)  # (n_batch, n_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06990420-61c6-47f4-a640-7c7ebedf124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize detector network\n",
    "detector = Detector(sample_rate, stft_params, n_bits, **config)\n",
    "print(f\"Detector parameters: {count_parameters(detector)}\")\n",
    "\n",
    "# Pass watermarked audio through detector to obtain watermark prediction\n",
    "pred = detector(signal).detach() > 0.5\n",
    "\n",
    "# Plot detection results; for untrained embedder/detector, we should see\n",
    "# random (~50%) accuracy in recovering watermark vector\n",
    "plt.plot(msg.flatten().tolist(), label=\"actual watermark\")\n",
    "plt.plot(pred.flatten().tolist(), label=\"predicted watermark\")\n",
    "plt.title(f\"Actual vs. predicted watermark vector (accuracy={(pred.flatten()==msg.flatten()).float().mean() :0.2f})\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7474f235-3d77-4b0f-af11-a5618a9c423e",
   "metadata": {},
   "source": [
    "## Training End-to-End\n",
    "\n",
    "Now that we have our embedder and detector networks, we want to train them jointly to embed and detect watermarks! We'll chain together the two steps above, passing the embedder's output to the detector as input, and then compute two losses: a __detection loss__ that encourages the detector to accurately identify the embedded watermark message, and a __perceptual transparency loss__ that encourages the embedder to produce watermarked audio that differs from the input as little as possible. The detection loss is computed on the detector's outputs, and its gradients flow through both the detector and embedder networks; the perceptual transparency loss is computed on the embedder's outputs, and its gradients flow only to the embedder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3c2b1b-9a35-4c52-aec4-8f4fe0ecd3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize networks\n",
    "embedder = Embedder(sample_rate, stft_params, n_bits, **config).to(device)\n",
    "detector = Detector(sample_rate, stft_params, n_bits, **config).to(device)\n",
    "\n",
    "print(f\"Embedder parameters: {count_parameters(embedder)}\")\n",
    "print(f\"Detector parameters: {count_parameters(detector)}\")\n",
    "\n",
    "opt_embedder = torch.optim.AdamW(embedder.parameters(), lr=3e-4)\n",
    "opt_detector = torch.optim.AdamW(detector.parameters(), lr=3e-4)\n",
    "\n",
    "# Load audio data\n",
    "loader = AudioLoader(sources=[DATA_DIR/\"LibriTTS_R/train-clean-100\"])\n",
    "dataset = AudioDataset(\n",
    "    loader, \n",
    "    sample_rate=sample_rate,\n",
    "    n_examples=1_000_000,       # Number of unique rows to load\n",
    "    duration=max_len_s,         # Pad/trim all audio to this duration\n",
    "    loudness_cutoff=-40,        # Sample random excerpts until minimum loudness (dB) cutoff is met\n",
    "    num_channels=1,             # If 1, downmix all audio to mono\n",
    "    without_replacement=False,  # Sample audio files with/without replacement\n",
    ")\n",
    "\n",
    "# Initialize data loader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb003c4-bc84-4f64-85e4-6882347dbb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_d = []\n",
    "loss_d = []\n",
    "loss_pt = []\n",
    "\n",
    "train_iter = iter(dataloader)\n",
    "\n",
    "pbar = tqdm(range(n_train_steps))\n",
    "for i in pbar:\n",
    "    \n",
    "    embedder.train()\n",
    "    detector.train()\n",
    "    \n",
    "    opt_embedder.zero_grad()\n",
    "    opt_detector.zero_grad()\n",
    "\n",
    "    try:\n",
    "        batch = next(train_iter)\n",
    "    except StopIteration:\n",
    "        train_iter = iter(dataloader)\n",
    "        batch = next(train_iter) \n",
    "\n",
    "    idx = batch[\"idx\"].to(device)\n",
    "    signal = batch[\"signal\"].to(device)                     # (n_batch, n_channels, n_samples)\n",
    "    signal_lengths = batch[\"signal_lengths\"].to(device)     # (n_batch,)\n",
    "    \n",
    "    # Sample random watermark message\n",
    "    msg = torch.randint(\n",
    "        0, 2, \n",
    "        (signal.batch_size, n_bits), \n",
    "        device=device\n",
    "    )  # (n_batch, n_bits)\n",
    "\n",
    "    # Embed message\n",
    "    watermarked = embedder(signal, msg, signal_lengths)  # (n_batch, n_channels, n_samples)\n",
    "\n",
    "    # Detect message\n",
    "    msg_pred = detector(watermarked, signal_lengths)  # (n_batch, n_bits)\n",
    "    \n",
    "    # Compute detection loss\n",
    "    _loss_d = torch.nn.functional.binary_cross_entropy(\n",
    "        msg_pred, msg.float(), reduction=\"none\",\n",
    "    ).mean(dim=-1)  # (n_batch,)\n",
    "    \n",
    "    # Compute perceptual transparency loss\n",
    "    _loss_pt = (\n",
    "        watermarked.magnitude - signal.magnitude\n",
    "    ).reshape(n_batch, -1).norm(dim=-1)  # (n_batch,)\n",
    "\n",
    "    # Combine losses\n",
    "    _loss = _loss_d + 0.01 * _loss_pt  # (n_batch,)\n",
    "\n",
    "    # Backward pass\n",
    "    _loss.mean().backward()\n",
    "    torch.nn.utils.clip_grad_norm_(embedder.parameters(), 1.0)\n",
    "    torch.nn.utils.clip_grad_norm_(detector.parameters(), 1.0)\n",
    "\n",
    "    # Update\n",
    "    opt_embedder.step()\n",
    "    opt_detector.step()\n",
    "\n",
    "    # Logging\n",
    "    with torch.no_grad():\n",
    "        acc_d += [((msg_pred > 0.5) == msg).float().mean().item()]\n",
    "        loss_d += [_loss_d.mean().item()]\n",
    "        loss_pt += [_loss_pt.mean().item()]\n",
    "\n",
    "    pbar.set_description(\n",
    "        f\"Detection accuracy: {acc_d[-1] :0.2f}, \"\n",
    "        f\"Detection loss: {loss_d[-1] :0.2f}, \"\n",
    "        f\"Perceptual transparency loss: {loss_pt[-1] :0.2f}, \"\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceac753-e9da-4351-b675-3c41902b425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_d)\n",
    "plt.title(\"Detection accuracy (training)\")\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(loss_d)\n",
    "plt.title(\"Detection loss (training)\")\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(loss_pt)\n",
    "plt.title(\"Perceptual transparency loss (training)\")\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6278d43b-4f2e-4140-8895-2ae0fb5fa5dc",
   "metadata": {},
   "source": [
    "We can now try out our trained watermark system on an unseen recording!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a2d6bb-0f2a-41d6-b368-d58ce32ad9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio\n",
    "signal = AudioSignal(ASSETS_DIR / \"audio\" / \"bryan_0.wav\").resample(sample_rate).to(device)\n",
    "signal = signal[..., :int(max_len_s * signal.sample_rate)]\n",
    "\n",
    "msg = torch.randint(0, 2, (1, n_bits), device=device)\n",
    "signal.clone().detach().cpu().widget()\n",
    "\n",
    "# Embed watermark\n",
    "watermarked = embedder(signal, msg)\n",
    "watermarked.clone().detach().cpu().widget()\n",
    "\n",
    "# Run detection on both original and watermarked signals\n",
    "msg_pred_unwatermarked = detector(signal) > 0.5\n",
    "msg_pred_watermarked = detector(watermarked) > 0.5\n",
    "\n",
    "# Evanluate perceptual transparency\n",
    "print(\n",
    "    f\"Watermark SNR: {snr(watermarked, signal).item() :0.2f}\\n\"\n",
    "    f\"Watermark SI-SDR: {si_sdr(watermarked, signal).item() :0.2f}\\n\"\n",
    ")\n",
    "\n",
    "# Plot detection results\n",
    "plt.plot(msg.flatten().tolist(), label=\"actual watermark\")\n",
    "plt.plot(msg_pred_unwatermarked.flatten().tolist(), label=\"predicted (unwatermarked)\")\n",
    "plt.plot(msg_pred_watermarked.flatten().tolist(), label=\"predicted (watermarked)\")\n",
    "plt.xlabel(\"Watermark message bit\")\n",
    "plt.title(\n",
    "    f\"Actual vs. predicted watermark vector\\n\"\n",
    "    f\"(unwatermarked accuracy={(msg_pred_unwatermarked.flatten()==msg.flatten()).float().mean() :0.2f})\\n\"\n",
    "    f\"(watermarked accuracy={(msg_pred_watermarked.flatten()==msg.flatten()).float().mean() :0.2f})\\n\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c838b1a5-35d8-4f40-8009-04c5beb680f5",
   "metadata": {},
   "source": [
    "## Robustness\n",
    "\n",
    "Let's put our trained watermarking system to the test in a more rigorous manner. We'll sample a large number of recordings unseen during training, sample corresponding random watermark messages, and compute the accuracy of messages recovered by our detector from both watermarked and unwatermarked audio. Using these accuracies as detections cores, we'll evaluate the performance of our system in terms of the achievable true positive rate at a fixed false positive rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab443c75-aac1-48d4-9f0b-48c31bdf2b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio data\n",
    "loader = AudioLoader(sources=[DATA_DIR/\"LibriTTS_R/test-clean\"])  # Test set!\n",
    "dataset = AudioDataset(\n",
    "    loader, \n",
    "    sample_rate=sample_rate,\n",
    "    n_examples=1_000,\n",
    "    duration=max_len_s,\n",
    "    loudness_cutoff=-40,     \n",
    "    num_channels=1,\n",
    "    without_replacement=True,\n",
    ")\n",
    "\n",
    "# Initialize data loader\n",
    "val_dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Watermarked and unwatermarked scores\n",
    "uwm_scores = []\n",
    "wm_scores = []\n",
    "\n",
    "pbar = tqdm(val_dataloader)\n",
    "for batch in pbar:\n",
    "    \n",
    "    embedder.eval()\n",
    "    detector.eval()\n",
    "\n",
    "    idx = batch[\"idx\"].to(device)\n",
    "    signal = batch[\"signal\"].to(device)                     # (n_batch, n_channels, n_samples)\n",
    "    signal_lengths = batch[\"signal_lengths\"].to(device)     # (n_batch,)\n",
    "    \n",
    "    # Sample random watermark message\n",
    "    msg = torch.randint(\n",
    "        0, 2, \n",
    "        (signal.batch_size, n_bits), \n",
    "        device=device\n",
    "    )  # (n_batch, n_bits)\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        # Embed message\n",
    "        watermarked = embedder(signal, msg)  # (n_batch, n_channels, n_samples)\n",
    "    \n",
    "        # Detect message\n",
    "        msg_pred_wm = detector(watermarked)  # (n_batch, n_bits)\n",
    "        msg_pred_uwm = detector(signal)      # (n_batch, n_bits)\n",
    "\n",
    "        acc_wm = ((msg_pred_wm > 0.5) == msg).float().mean(dim=-1)    # (n_batch,)\n",
    "        acc_uwm = ((msg_pred_uwm > 0.5) == msg).float().mean(dim=-1)  # (n_batch,)\n",
    "\n",
    "        wm_scores += [acc_wm]\n",
    "        uwm_scores += [acc_uwm]\n",
    "\n",
    "wm_scores = torch.cat(wm_scores, dim=0)\n",
    "uwm_scores = torch.cat(uwm_scores, dim=0)\n",
    "\n",
    "print(\n",
    "    f\"TPR @ 10% FPR: {tpr_at_fpr(wm_scores, uwm_scores, 0.10) :0.2f}\\n\"\n",
    "    f\"TPR @ 1% FPR: {tpr_at_fpr(wm_scores, uwm_scores, 0.01) :0.2f}\\n\"\n",
    "    f\"TPR @ 0.1% FPR: {tpr_at_fpr(wm_scores, uwm_scores, 0.001) :0.2f}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f5f121-a913-4787-9525-39de67688f0e",
   "metadata": {},
   "source": [
    "Not too shabby for a bare-bones end-to-end system! And importantly, we didn't need to hand-craft a clever embedding or detections scheme -- we let our networks learn everything for us. Now what happens if we perturb the watermarked audio before detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02798263-f514-4931-a064-447c2d15486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv(\n",
    "    audio_files=list((Path(NOISE_DIR)).rglob(\"*.wav\")),\n",
    "    output_csv=MANIFESTS_DIR / \"noise_room.csv\", \n",
    ")\n",
    "create_csv(\n",
    "    audio_files=list((Path(RIR_DIR)).rglob(\"*.wav\")),\n",
    "    output_csv=MANIFESTS_DIR / \"rir_real.csv\",\n",
    ")\n",
    "\n",
    "noise = BackgroundNoise(\n",
    "    snr=(\"uniform\", 20.0, 30.0),  # Sample noise level uniformly in [10, 30]dB\n",
    "    sources=[MANIFESTS_DIR / \"noise_room.csv\"],\n",
    "    eq_amount=(\"const\", 1.0),     # Sample EQ level as a fixed value of 1.0\n",
    "    n_bands=3,\n",
    "    prob=1.0,\n",
    "    loudness_cutoff=None,\n",
    ")\n",
    "\n",
    "reverb = RoomImpulseResponse(\n",
    "    drr=(\"uniform\", 10.0, 30.0),   # Sample reverb direct-reverberant ratio uniformly in [0, 30]dB\n",
    "    sources=[MANIFESTS_DIR / \"rir_real.csv\"],\n",
    "    eq_amount=(\"const\", 1.0),     # Sample EQ level as a fixed value of 1.0\n",
    "    n_bands=6,\n",
    "    prob=1.0,\n",
    "    use_original_phase=False,\n",
    "    offset=0.0,\n",
    "    duration=1.0,\n",
    ")\n",
    "\n",
    "noise_and_reverb = Compose(noise, reverb)\n",
    "\n",
    "# Watermarked and unwatermarked scores\n",
    "uwm_scores = []\n",
    "wm_scores = []\n",
    "\n",
    "pbar = tqdm(val_dataloader)\n",
    "for batch in pbar:\n",
    "    \n",
    "    embedder.eval()\n",
    "    detector.eval()\n",
    "\n",
    "    idx = batch[\"idx\"].to(device)\n",
    "    signal = batch[\"signal\"].to(device)                     # (n_batch, n_channels, n_samples)\n",
    "    signal_lengths = batch[\"signal_lengths\"].to(device)     # (n_batch,)\n",
    "    \n",
    "    # Sample random watermark message\n",
    "    msg = torch.randint(\n",
    "        0, 2, \n",
    "        (signal.batch_size, n_bits), \n",
    "        device=device\n",
    "    )  # (n_batch, n_bits)\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        # Embed message\n",
    "        watermarked = embedder(signal, msg, signal_lengths)  # (n_batch, n_channels, n_samples)\n",
    "\n",
    "        # Apply randomized transformations\n",
    "        tfm_kwargs = noise_and_reverb.batch_instantiate(idx.tolist(), signal)\n",
    "        tfm_signal = noise_and_reverb.transform(signal.clone().cpu(), **tfm_kwargs).to(device)\n",
    "        tfm_watermarked = noise_and_reverb.transform(watermarked.clone().cpu(), **tfm_kwargs).to(device)\n",
    "        \n",
    "        # Detect message\n",
    "        msg_pred_wm = detector(tfm_watermarked)  # (n_batch, n_bits)\n",
    "        msg_pred_uwm = detector(tfm_signal)      # (n_batch, n_bits)\n",
    "\n",
    "        acc_wm = ((msg_pred_wm > 0.5) == msg).float().mean(dim=-1)    # (n_batch,)\n",
    "        acc_uwm = ((msg_pred_uwm > 0.5) == msg).float().mean(dim=-1)  # (n_batch,)\n",
    "\n",
    "        wm_scores += [acc_wm]\n",
    "        uwm_scores += [acc_uwm]\n",
    "\n",
    "wm_scores = torch.cat(wm_scores, dim=0)\n",
    "uwm_scores = torch.cat(uwm_scores, dim=0)\n",
    "\n",
    "print(\n",
    "    f\"TPR @ 10% FPR: {tpr_at_fpr(wm_scores, uwm_scores, 0.10) :0.2f}\\n\"\n",
    "    f\"TPR @ 1% FPR: {tpr_at_fpr(wm_scores, uwm_scores, 0.01) :0.2f}\\n\"\n",
    "    f\"TPR @ 0.1% FPR: {tpr_at_fpr(wm_scores, uwm_scores, 0.001) :0.2f}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000a5ade-54fa-408d-ac45-eaecd5a02681",
   "metadata": {},
   "source": [
    "Here, we can see that our approach fails under noise and reverb transformations. And that shouldn't be a surprise: we did not train our system to be robust to any transformations! Let's try again, but this time we'll incorporate transformations directly into our training loop. To ensure we aren't \"training on the test\" distribution, we'll used simplified simulations of background noise and reverberation (implemented in `wm_tutorial/tfm.py`) that can be run in parallel on the GPU without loading any files from disk. For reverberation, we'll use the noise-shaped impulse response implementation from [`dasp-pytorch`](https://github.com/csteinmetz1/dasp-pytorch/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c9d418-14a7-4c7e-998c-bb40c4dae9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize networks\n",
    "embedder = Embedder(sample_rate, stft_params, n_bits, **config).to(device)\n",
    "detector = Detector(sample_rate, stft_params, n_bits, **config).to(device)\n",
    "\n",
    "print(f\"Embedder parameters: {count_parameters(embedder)}\")\n",
    "print(f\"Detector parameters: {count_parameters(detector)}\")\n",
    "\n",
    "opt_embedder = torch.optim.AdamW(embedder.parameters(), lr=3e-4)\n",
    "opt_detector = torch.optim.AdamW(detector.parameters(), lr=3e-4)\n",
    "\n",
    "# Load audio data\n",
    "loader = AudioLoader(sources=[DATA_DIR/\"LibriTTS_R/train-clean-100\"])\n",
    "dataset = AudioDataset(\n",
    "    loader, \n",
    "    sample_rate=sample_rate,\n",
    "    n_examples=1_000_000,       # Number of unique rows to load\n",
    "    duration=max_len_s,         # Pad/trim all audio to this duration\n",
    "    loudness_cutoff=-40,        # Sample random excerpts until minimum loudness (dB) cutoff is met\n",
    "    num_channels=1,             # If 1, downmix all audio to mono\n",
    "    without_replacement=False,  # Sample audio files with/without replacement\n",
    ")\n",
    "\n",
    "# Initialize data loader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Training transforms\n",
    "fast_noise = Noise(\n",
    "    snr=(\"uniform\", 10.0, 30.0),\n",
    "    eq_amount=(\"const\", 1.0),\n",
    "    prob=0.5,\n",
    ")\n",
    "eq = Equalizer(\n",
    "    n_bands=6,\n",
    "    prob=0.5,\n",
    ")\n",
    "speed = Speed(\n",
    "    factor=(\"choice\", (0.95, 0.96, 0.97, 0.98, 0.99, 1.01, 1.02, 1.03, 1.04, 1.05)),\n",
    "    prob=0.5,\n",
    ")\n",
    "train_tfm = Compose(speed, fast_noise, eq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b59113e-1d63-42ca-bfac-a6c2318f91b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_d = []\n",
    "loss_d = []\n",
    "loss_pt = []\n",
    "\n",
    "train_iter = iter(dataloader)\n",
    "\n",
    "pbar = tqdm(range(n_train_steps))\n",
    "for i in pbar:\n",
    "    \n",
    "    embedder.train()\n",
    "    detector.train()\n",
    "    \n",
    "    opt_embedder.zero_grad()\n",
    "    opt_detector.zero_grad()\n",
    "\n",
    "    try:\n",
    "        batch = next(train_iter)\n",
    "    except StopIteration:\n",
    "        train_iter = iter(dataloader)\n",
    "        batch = next(train_iter) \n",
    "\n",
    "    idx = batch[\"idx\"].to(device)\n",
    "    signal = batch[\"signal\"].to(device)                     # (n_batch, n_channels, n_samples)\n",
    "    signal_lengths = batch[\"signal_lengths\"].to(device)     # (n_batch,)\n",
    "    \n",
    "    # Sample random watermark message\n",
    "    msg = torch.randint(\n",
    "        0, 2, \n",
    "        (signal.batch_size, n_bits), \n",
    "        device=device\n",
    "    )  # (n_batch, n_bits)\n",
    "\n",
    "    # Embed message\n",
    "    watermarked = embedder(signal, msg, signal_lengths)  # (n_batch, n_channels, n_samples)\n",
    "\n",
    "    # Apply transformations\n",
    "    tfm_kwargs = train_tfm.batch_instantiate(idx.tolist(), watermarked)\n",
    "    tfm_watermarked = train_tfm.transform(watermarked.clone(), **tfm_kwargs)\n",
    "    \n",
    "    # Detect message\n",
    "    msg_pred = detector(tfm_watermarked, signal_lengths)  # (n_batch, n_bits)\n",
    "    \n",
    "    # Compute detection loss\n",
    "    _loss_d = torch.nn.functional.binary_cross_entropy(\n",
    "        msg_pred, msg.float(), reduction=\"none\",\n",
    "    ).mean(dim=-1)  # (n_batch,)\n",
    "    \n",
    "    # Compute perceptual transparency loss\n",
    "    _loss_pt = (\n",
    "        watermarked.magnitude - signal.magnitude\n",
    "    ).reshape(n_batch, -1).norm(dim=-1)  # (n_batch,)\n",
    "\n",
    "    # Combine losses; apply \"curriculum\" to down-weight perceptual transparency \n",
    "    # loss to prevent it from dominating detection loss, which is trickier to\n",
    "    # optimize when training through simulated audio transformations\n",
    "    _loss = _loss_d + min(0.001, 0.0001 + (0.000001 * i)) * _loss_pt  # (n_batch,)\n",
    "\n",
    "    # Backward pass\n",
    "    _loss.mean().backward()\n",
    "    torch.nn.utils.clip_grad_norm_(embedder.parameters(), 1.0)\n",
    "    torch.nn.utils.clip_grad_norm_(detector.parameters(), 1.0)\n",
    "\n",
    "    # Update\n",
    "    opt_embedder.step()\n",
    "    opt_detector.step()\n",
    "\n",
    "    # Logging\n",
    "    with torch.no_grad():\n",
    "        acc_d += [((msg_pred > 0.5) == msg).float().mean().item()]\n",
    "        loss_d += [_loss_d.mean().item()]\n",
    "        loss_pt += [_loss_pt.mean().item()]\n",
    "\n",
    "    pbar.set_description(\n",
    "        f\"Detection accuracy: {acc_d[-1] :0.2f}, \"\n",
    "        f\"Detection loss: {loss_d[-1] :0.2f}, \"\n",
    "        f\"Perceptual transparency loss: {loss_pt[-1] :0.2f}, \"\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20554263-bb5b-4069-b8bd-1c44eab03612",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_d)\n",
    "plt.title(\"Detection accuracy (training)\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(loss_d)\n",
    "plt.title(\"Detection loss (training)\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(loss_pt)\n",
    "plt.title(\"Perceptual transparency loss (training)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37169312-f16a-421e-bb4f-005e276960e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio\n",
    "signal = AudioSignal(ASSETS_DIR / \"audio\" / \"bryan_0.wav\").resample(sample_rate).to(device)\n",
    "signal = signal[..., :int(max_len_s * signal.sample_rate)]\n",
    "\n",
    "msg = torch.randint(0, 2, (1, n_bits), device=device)\n",
    "signal.clone().detach().cpu().widget()\n",
    "\n",
    "# Embed watermark\n",
    "watermarked = embedder(signal, msg)\n",
    "watermarked.clone().detach().cpu().widget()\n",
    "(signal - watermarked).clone().detach().cpu().widget()\n",
    "\n",
    "# Run detection on both original and watermarked signals\n",
    "msg_pred_unwatermarked = detector(signal) > 0.5\n",
    "msg_pred_watermarked = detector(watermarked) > 0.5\n",
    "\n",
    "# Evanluate perceptual transparency\n",
    "print(\n",
    "    f\"Watermark SNR: {snr(watermarked, signal).item() :0.2f}\\n\"\n",
    "    f\"Watermark SI-SDR: {si_sdr(watermarked, signal).item() :0.2f}\\n\"\n",
    ")\n",
    "\n",
    "# Plot detection results\n",
    "plt.plot(msg.flatten().tolist(), label=\"actual watermark\")\n",
    "plt.plot(msg_pred_unwatermarked.flatten().tolist(), label=\"predicted (unwatermarked)\")\n",
    "plt.plot(msg_pred_watermarked.flatten().tolist(), label=\"predicted (watermarked)\")\n",
    "plt.xlabel(\"Watermark message bit\")\n",
    "plt.title(\n",
    "    f\"Actual vs. predicted watermark vector\\n\"\n",
    "    f\"(unwatermarked accuracy={(msg_pred_unwatermarked.flatten()==msg.flatten()).float().mean() :0.2f})\\n\"\n",
    "    f\"(watermarked accuracy={(msg_pred_watermarked.flatten()==msg.flatten()).float().mean() :0.2f})\\n\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355f2644-5cbd-40f1-90dd-0dca4a35f6d6",
   "metadata": {},
   "source": [
    "Now that we've trained with transformations, let's see if we do any better on our robustness evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db1ccd-7737-4f1b-84fb-a978f45aefdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watermarked and unwatermarked scores\n",
    "uwm_scores = []\n",
    "wm_scores = []\n",
    "\n",
    "pbar = tqdm(val_dataloader)\n",
    "for batch in pbar:\n",
    "    \n",
    "    embedder.eval()\n",
    "    detector.eval()\n",
    "\n",
    "    idx = batch[\"idx\"].to(device)\n",
    "    signal = batch[\"signal\"].to(device)                     # (n_batch, n_channels, n_samples)\n",
    "    signal_lengths = batch[\"signal_lengths\"].to(device)     # (n_batch,)\n",
    "    \n",
    "    # Sample random watermark message\n",
    "    msg = torch.randint(\n",
    "        0, 2, \n",
    "        (signal.batch_size, n_bits), \n",
    "        device=device\n",
    "    )  # (n_batch, n_bits)\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        # Embed message\n",
    "        watermarked = embedder(signal, msg, signal_lengths)  # (n_batch, n_channels, n_samples)\n",
    "\n",
    "        # Apply randomized transformations\n",
    "        tfm_kwargs = noise_and_reverb.batch_instantiate(idx.tolist(), signal)\n",
    "        tfm_signal = noise_and_reverb.transform(signal.clone().cpu(), **tfm_kwargs).to(device)\n",
    "        tfm_watermarked = noise_and_reverb.transform(watermarked.clone().cpu(), **tfm_kwargs).to(device)\n",
    "        \n",
    "        # Detect message\n",
    "        msg_pred_wm = detector(tfm_watermarked)  # (n_batch, n_bits)\n",
    "        msg_pred_uwm = detector(tfm_signal)      # (n_batch, n_bits)\n",
    "\n",
    "        acc_wm = ((msg_pred_wm > 0.5) == msg).float().mean(dim=-1)    # (n_batch,)\n",
    "        acc_uwm = ((msg_pred_uwm > 0.5) == msg).float().mean(dim=-1)  # (n_batch,)\n",
    "\n",
    "        wm_scores += [acc_wm]\n",
    "        uwm_scores += [acc_uwm]\n",
    "\n",
    "wm_scores = torch.cat(wm_scores, dim=0)\n",
    "uwm_scores = torch.cat(uwm_scores, dim=0)\n",
    "\n",
    "print(\n",
    "    f\"TPR @ 10% FPR: {tpr_at_fpr(wm_scores, uwm_scores, 0.10) :0.2f}\\n\"\n",
    "    f\"TPR @ 1% FPR: {tpr_at_fpr(wm_scores, uwm_scores, 0.01) :0.2f}\\n\"\n",
    "    f\"TPR @ 0.1% FPR: {tpr_at_fpr(wm_scores, uwm_scores, 0.001) :0.2f}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1be7a1-5744-4427-bfee-c7f38bddca2e",
   "metadata": {},
   "source": [
    "## A Final Note on Tradeoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da07ec49-92e7-454d-a428-fa4f21b18680",
   "metadata": {},
   "source": [
    "We were able to improve our robustness, but with some notable costs:\n",
    "* Training took longer due to processing from our simulated transformations; this can sometimes be reduced with more efficient implementations, but generally slows things down\n",
    "* Our \"robust\" watermark was more clearly audible than our original watermark; in fact, we needed to use a \"curriculum\" to down-weight our perceptual transparency loss to allow our watermark to be \"loud\" enough to withstand audio transformations!\n",
    "\n",
    "\n",
    "When training end-to-end watermarking systems, we need to take into account the inherent trade-offs between robustness, perceptual transparency, and capacity. For example, trying to encode longer watermark messages (i.e. _increasing capacity_) typically comes at a cost to transparency or robustness -- you can try increasing `n_bits` in this notebook and see for yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d953a977-3839-48a7-9195-10b3c800743a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genmodels",
   "language": "python",
   "name": "genmodels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
